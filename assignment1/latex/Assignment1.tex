\documentclass[11pt,a4paper,landscape]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{listings}
\author{jacmalm@kth.se}
\title{DD2343 Assignment 1}
\begin{document}

\maketitle
\newpage
\section{Principal Component Analysis}

\subsection{Explain why data-centering is required before performing PCA.  What might happen if we perform PCA on non-centered data?}

PCA attempts to find new axes that are linear combinations of the old axes, such that these new axes, or principal components, minimize the mean squared distance between the original data points and their projection onto the principal axes \cite{pca_lecture_book}.  One key thing to note is that these principal components are limited by the need to cross through the origin. This limitation means that data that is shifted cannot be approximated as well. A simple example is to consider the points $\lbrace-1, 1\rbrace$, $\lbrace0,0,\rbrace$, $\lbrace1, -1\rbrace$. These points all lie perfectly along a 1 dimensional manifold, the line with slope -1 going through the origin. The mean of this dataset is 0. Thus, we expect the first principle component to consist of this line, and we expect all of the variance to be explained by the first principle component, as the PCA model is perfectly respected.When we run the PCA code (with the lines responsible for centering the data commented out in the SKLearn library file) given in appendix 1, this is exactly what we see. Great! PCA works.\newline

However, if we shift the location of this manifold, while keeping its shape intact, this will change. If we now consider the points $\lbrace49, 51\rbrace$, $\lbrace50,50\rbrace$, $\lbrace51, 49\rbrace$, it is easy to see that they also perfectly lie along a line, indeed a line with the same slope as the previous example. However, this line does not pass through the origin, and will thus not be found by PCA. Instead, what we expect to happen, is that the first principle component will be a line that goes through the origin and minimizes the distance from the data points and their projection onto the first principle axis. As the data is symmetric around their mean, we expect this to be a line going through the mean point,$\lbrace50,50\rbrace$, and the origin.This is a line with slope 1. In fact, this line is perpendicular to the line that the data points lie along. When looking at the explained variance, we can also see that this line does not explain all of the variance, necessitating us to use both principal components to perfectly be able to get back all of our data, meaning that PCA fails to detect the inherent 1-dimensionality of the dataset. Again, these conclusions are verifiable with provided code in Appendix 1, and visually demonstrated in figure 1 and figure 2.

\includegraphics[width=\textwidth]{figure1_2.pdf}

\subsection{Is a single SVD operation sufficient to performing PCA on both the rows and columns of a data matrix}

Given a mean-centered data matrix \textbf{X} with \textit{m} columns and \textit{n} rows, $m < n$, we know that we can calculate the covariance matrix \textbf{$C_{X}$} in the following manner:
$$ C_{X} = \frac{1}{n} X^{T}X$$
We know that this covariance matrix will be symmetric, meaning that it has an eigendecomposition where the eigenvectors are orthogonal, resulting in:

$$ C_{X} = Q \Lambda Q^{T} $$

This decomposition can be interpreted as capturing the patterns of variance across the dataset \textbf{X}. The columns in \textbf{Q} provide directions, and their corresponding eigenvalues (as a proportion of the sum of all eigenvalues) encode how much of the variance is captured in the direction of the eigenvector. Thus, the eigenvectors in this decomposition are the principal axes we use to translate our datapoints into our PCA projection \cite{pca_from_cxx} .\newline

If we rewrite $X$ in terms of its SVD  before computing the covariance matrix, we see that it can be written as

$$ C_{X} = \frac{1}{n} V \Sigma U^{T} U\Sigma^{T} V^{T}  $$

Since \textbf{U} and \textbf{V} are orthonormal, and the only non-zero points of $ \Sigma $ lie along its first diagonal, this expression cancels out to                                                                                                                                                                          

$$ C_{X} = \frac{1}{n} V \widehat{\Sigma}^{2} V^{T}  $$

Allowing us to draw the following conclusions
$$ Q = V $$

$$ \Lambda =  \frac{1}{n} \widehat{\Sigma}^{2}$$

Where $\widehat{\Sigma}$ is a square, $n \times n$ matrix, where there are \textit{m} non-zero values all placed along the first diagonal.\newline

That is, the principle axes are given by \textbf{V} when the corresponding singular values are sorted in order of magnitude.

If we instead want to find the principle axes of the transpose of X, this turns into

$$ C_{X^{T}} = \frac{1}{n} U \Sigma^{T} V^{T} V\Sigma U^{T}  $$
$$ C_{X^{T}} = \frac{1}{n} U \overline{\Sigma}^{2} U^{T}  $$

Where $\overline{\Sigma}$ is a square, $m \times m$ diagonal matrix.\newline


If we restrict ourselves to looking at non-zero values (the first \textit{m} values along the first diagonal) $\widehat{\Sigma} = \overline{\Sigma} $, these are the only values that will affect our principle axes.\newline

Thus we can conclude that the principle axes that define our PCA projection of the transpose of $X$ are given by $U$. This is the same $U$ we get from performing a SVD on $X$, which means that we get all the necessary information for performing PCA on $X$ and $X^{T}$ from one SVD operation. We will  need to pad $\Sigma$ with zeroes in order to make the matrix multiplication possible, and we will need to transpose one of the terms we get from the first SVD operation,  but we only need to perform it once.\newline

A program which verifies that the principal components of $X$ can be retrieved from an SVD of $X^{T}$ is given in Appendix 2.

\subsection{Explain why the use of the pseudo-inverse is a good choice to obtain the inverse mapping of the linear map}

The pseudo-inverse is a good choice to obtain the inverse mapping because when the PCA model is fully respected, that is our data points $\textbf{y} \in \mathbb{R}^d$ are fully contained within a subspace $\textbf{U} \in \mathbb{R}^k$ where $k < d$, and \textbf{U} is the column space of \textbf{W}, $\textbf{W}^+$ is the inverse mapping of \textbf{W}, even when \textbf{W} is non-square.\newline

However, when the PCA model is not fully respected and there are points in \textbf{y} that lie outside of \textbf{U}, the pseudo-inverse computes the inverse mapping from the points projected onto \textbf{U}, which is the approximation that minimizes the euclidean distance between computed and actual points\cite{pseudoinverse}.\newline

Thus, using the pseudo-inverse of the linear transformation that is hypothesized to have generated \textbf{y} from \textbf{x}, allows us to compute the projection onto the PCA axes even for points that do not completely lie within the $k$ dimensional column space, which often occurs due to noisy measurements or an underlying model not quite aligned with the PCA assumptions.


\section{Multidimensional Scaling and Isomap}

\subsection{Argue that the process to obtain the neighbourhood graph G in the Isomap method may yield a disconnected graph.  Provide an example.  Explain why this is problematic.}

The Isomap algorithm works similarly to MDS in that it relies on computing a Gram matrix S from a distance matrix D. However,dissimilarly to MDS,this distance matrix is not made up of euclidean distances. Instead, the distances are approximations of the shortest path from point A to point B where the path is restricted to the theoretical manifold our data distribution makes up. Computing the shape of this manifold as well as the length of the path through the manifold from point A to B is infeasible. An approximation of the manifold can be created by drawing a neighbourhood graph G. This neighbourhood graph G is created one of two ways. Either by joining each point to its K nearest neighbours, where a point A is considered closer to point B than to C if the euclidean distance between A-B is smaller than A-C.  Alternatively the neighbourhood graph can be constructed by choosing to join a point A to all of its neighbours that lie within a certain distance \textit{d}.\newline


To demonstrate how this process can lead to a disconnected graph,  assume we have the following nodes located at given coordinates.

\begin{tikzpicture}[node distance=15mm, main/.style = {draw, circle}] 
\node[main] (1) {$0,0$}; 
\node[main] (2)[right of = 1] {$0,1$}; 
\node[main] (3)[right of = 2] {$0,2$}; 
\node[main] (4)[right of = 3] {$0,10$}; 
\node[main] (5)[right of = 4] {$0,11$}; 
\node[main] (6)[right of = 5] {$0,12$}; 

\end{tikzpicture}

If we go by the k-nearest neighbour approach and set k=2, or go by max euclidean distance between nodes and set \textit{d} to smaller than 8, our resulting graph will be disconnected.

\begin{tikzpicture}[node distance=15mm, main/.style = {draw, circle}] 
\node[main] (1) {$0,0$}; 
\node[main] (2)[right of = 1] {$0,1$}; 
\node[main] (3)[right of = 2] {$0,2$}; 
\node[main] (4)[right of = 3] {$0,10$}; 
\node[main] (5)[right of = 4] {$0,11$}; 
\node[main] (6)[right of = 5] {$0,12$}; 
\draw(1) -- (2);
\draw(2) -- (3);
\draw(4) -- (5);
\draw(5) -- (6);
\end{tikzpicture}

According to the Isomap algorithm, when we compute the distance matrix, the entry between point A and point B will be the summation of the distances between all nodes that lie along the shortest path between A and B through the neighbourhood graph G constructed in the previous step. If this is a disconnected graph, the distance between any point A to any point B where A and B are members of different disjoint subsets in G will be infinite, or undefined. In either of these cases the spectral decomposition of the Gram matrix corresponding to the distance matrix will be undefined and the Isomap algorithm will fail.

\bibliography{mybib}
\bibliographystyle{plain}

\section{Appendices}
\subsection{Appendix 1}
\lstinputlisting[language=python]{../code/centering_pca.py}
\subsection{Appendix 2}
\lstinputlisting[language=python]{../code/pca_transpose.py}

\end{document}